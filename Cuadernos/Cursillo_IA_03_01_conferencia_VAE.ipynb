{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<figure>\n",
    "<img src=\"../imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "    <img src=\"../imagenes/Foto Alvaro Montenegro.png\" width=\"40\" height=\"40\" align=\"right\" /> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:#4361EE\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Simposio Internacional de Estadística 2024</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"><center>Minicurso de Inteligencia Artificial Moderna III<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><center>Modelos Generadores. VAE<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/vae-diagram-1-1024x563.jpg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [LearnOpenCV](https://learnopencv.com/variational-autoencoder-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color:#4361EE\">Profesor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Alvaro  Mauricio Montenegro Díaz, Ph.D.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a2013fd2-1aea-4603-be27-13a595967673"
   },
   "source": [
    "## <span style=\"color:#4361EE\">Contenido</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El problema](#El-problema)\n",
    "* [Inferencia Variacional](#Inferencia-Variacional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autocodificadores variacionales constituyen una herramienta para problemas en los que se supone que existe una variable latente que genera los datos de entrada.\n",
    "\n",
    "Para fijar ideas, supongamos que se tiene el resultado de una prueba académica aplicada a una muestra de personas. Más aún, supongamos que la codificación es binaria, en donde 1 (uno) codifica una respuesta correcta y 0 (cero) una respuesta incorrecta. \n",
    "\n",
    "Observe que la representación vectorial de la base de datos *FashionMnist*, puede considerarse similar si cada pixel se codifica como cero o uno únicamente.\n",
    "\n",
    "Entonces a la entrada se tienen vectores binarios  $\\mathbf{x}$. de tamaño fijo, digamos $d=100$. Se busca entonces una respresentación de estos vectores en un espacio Euclidiano de dimensión $m$, en donde $m<<d$. Digamos $m=5$. \n",
    "\n",
    "Los expertos utilizan distintas técnicas como por ejemplo el empleo del análisis de componentes principales (ACP) o la $q$-dimensión, para detectar la dimensión $m$.\n",
    "\n",
    "La imagen  ilustra el procedimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Probabilistico_intro.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Implementación de un  Autoencoder Variacional para una prueba de Estado</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4361EE\">El problema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el espacio de entrada se tiene vectores de respuestas, Cada uno de ellos denotado por $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "Matemáticamente, estos objetos están en el espacio $ A= \\{0,1\\}^d$. Es decir, sus elementos son vectores de tamaño $d$ y las componentes de cada vector son unos y ceros. La respuestas en la prueba de una  determinada persona se codifican en un vector $\\mathbf{x}\\in A$. Sobre $A$ se supone definida una medida de probabilidad que genéricamente denotaremos $P_{\\theta}(\\mathbf{x})$. Entonces se asume que  $\\mathbf{x}$ es una muestra generada por dicha medida de probabilidad y se escribe $\\mathbf{x}\\sim P_{\\theta}$. El símbolo $\\theta$ refiere a los parámetros (desconocidos) de la medida de probabilidad.\n",
    "\n",
    "Ahora bien, se supone que la capacidad de una persona para resolver la prueba puede ser representada por un vector aleatorio $\\mathbf{z} \\in \\mathbb{R}^m$. En principio, la teoría psicométrica está orientada a predecir un valor para $\\mathbf{z}$, que en una prueba de Estado corresponde a una calificación.\n",
    "\n",
    "No haremos eso aquí, pero si generaremos muestras que correspondería a distintos valores que podría representar la capacidad de la persona. Estos valores se llaman muestras de las variables latentes. \n",
    "\n",
    "\n",
    "Si ahora se considera a un individuo en particular que  tiene capacidad $\\mathbf{z}$ para resolver la prueba y obtiene el vector de respuestas $\\mathbf{x}$, claramente existe una relación entre esos dos vectores que desde el punto de vista probabilístico se escribe de la siguiente forma:\n",
    "\n",
    "1. El vector de respuestas $\\mathbf{x}$ es generado a partir de la  distribución condicional $P(\\mathbf{x}|\\mathbf{z})$. En términos estadísticos $P(\\mathbf{x}|\\mathbf{z})$ es conocida excepto por sus parámetros. Estimar los parámetros de esta distribución será el problema central de una `autoencoder variacional` y es la parte que corresponde al decodificador. Aquí por lo general se introduce un modelo estadístico, por ejemplo un modelo de Bernoulli, y se estiman sus parámetros, haciendo uso de la entropía cruzada como función de pérdida.\n",
    "\n",
    "2. La variable latente $\\mathbf{z}$ es generada por la distribución condicional $Q(\\mathbf{z}|\\mathbf{x})$. Esta es siempre desconocida y justamente el problema de la inferencia variacional es encontrar una distribución simple, Gaussiana por ejemplo, que aproxime lo mejor posible a $Q(\\mathbf{z}|\\mathbf{x})$. Desde el punto de vista del autoencoder variacional, esta parte corresponde al codificador.\n",
    "\n",
    "\n",
    "En la siguiente sección se muestra el desarrollo teórico del problema de inferencia variacional. Solamente para lectores con algún conocimiento matemático. Puede omitir sin problema. \n",
    "\n",
    "* [Regresar al inicio](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4361EE\">Inferencia Variacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de la inferencia variacional es aproximar una densidad haciendo una paso intermedio por un espacio de variables latentes.\n",
    "\n",
    "El proceso  puede ser imaginado de una forma análoga a la construcción de un codificador en variables latentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Planteamiento del problema variacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que $p_{\\theta}(\\mathbf{x})$ es la densidad asociada a un vector aleatorio de respuesta. El problema estadístico en principio es estimar el parámetro $\\theta$ que indexa a la distribución.\n",
    "\n",
    "Si se asume que $\\mathbf{z}$ es el vector latente asociado a $\\mathbf{x}$  se tiene que\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x})= \\int P_{\\theta}(\\mathbf{z},\\mathbf{x})d\\mathbf{z} = \\int P_{\\theta}(\\mathbf{z}|\\mathbf{x})P(\\mathbf{x})d\\mathbf{z}.\n",
    "$$\n",
    "\n",
    "Un esquema de muestreo para $\\mathbf{z}$ se deriva de esta ecuación:\n",
    "\n",
    "1. Dada una muestra $\\mathbf{x} \\sim P_{\\theta}(x)$, es decir una muestra en la entrada de la red.\n",
    "2. Se obtiene una muestra $\\mathbf{z}\\sim P_{\\theta}(\\mathbf{z}|\\mathbf{x})$.\n",
    "\n",
    "\n",
    "El problema es que en general $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$ es intratable. Lo que vamos a hacer para obtener la muestras $\\mathbf{z}$ es obtener una densidad aproximada $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ en una familia conocida.\n",
    "\n",
    "En esta lección lo que haremos es proponer una aproximación del tipo $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})= \\mathcal{N}_m(\\boldsymbol{\\mu}(\\mathbf{x}),\\text{diag}(\\boldsymbol{\\sigma}^2(\\mathbf{x}))) $.\n",
    "\n",
    "El problema en la inferencia variacional es justamente encontrar una buena distribución aproximante $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ talque \n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\approx P_{\\theta}(\\mathbf{z}|\\mathbf{x}).\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Aproximación Variacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el propósito de convertir $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ en una función de densidad tratable la solución propuesta desde la inferencia variacional es la introducción de una densidad aproximada $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ de tal manera que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\approx P_{\\theta}(\\mathbf{z}|\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "La densidad $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ se escoge en una familia de distribuciones tratables indexadas por $\\phi$. Es común escoger $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ en la familia normal multivariada. Eso haremos en esta lección. \n",
    "\n",
    "Entonces para cada $\\mathbf{x}$ tendremos que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}^2(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{\\mu}(\\mathbf{x})$ es el vector de medias (condicionadas por $\\mathbf{x}$) y $\\boldsymbol{\\sigma}(\\mathbf{x})$ es un vector de desviaciones estándar. Como la matriz de covarianza es diagonal, se está asumiendo que las componentes del vector $\\mathbf{z}$ son condicionalmente independientes, dado el vector  $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Divergencia Kullback-Leibler (KL)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha definido la familia de distribuciones a partir de la cual se obtendrá la aproximación $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ el siguiente paso es decidir como medir la proximidad o la discrepancia de la densidad aproximante con la densidad original. La solución sugerida desde la inferencia variacional es usar la divergencia KL, la cual se define por\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)) = \\int(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)Q_{\\phi}(\\mathbf{z}|\\mathbf{x})dz.\n",
    "$$\n",
    "\n",
    "El símbolo $\\mathbb{E}_{\\phi}$ indica que la esperanza es con respecto a la densidad $Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Cota inferior de la evidencia (ELBO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo en la inferencia variacional es encontrar una densidad aproximante $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$  para la densidad $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ utilizando como métrica la divergencia KL, que por cierto no es una distancia, dado que no es simétrica.\n",
    "\n",
    "A partir del teorema de Bayes se obtiene que \n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{z}|\\mathbf{x})= \\frac{P_{\\theta}(\\mathbf{x}|\\mathbf{z})P_{\\theta}(\\mathbf{z})}{P_{\\theta}(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Por lo que la divergencia KL se transforma en\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x}))  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})- \\log P_{\\theta}(\\mathbf{z})\\right)) + \\log P_{\\theta}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "De donde se obtiene que\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] = \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "$$\n",
    "\n",
    "Esta ecuación constituye el núcleo de la inferencia variacional. El lado izquierdo de la ecuación  contiene el término $P_{\\theta}(\\mathbf{x})$ que se busca maximizar menos el error de la aproximación medido por $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})]$ que se espera  sea aproximadamente cero. \n",
    "\n",
    "\n",
    "Se sabe que la divergencia KL siempre es positiva, por lo que la parte izquierda de la ecuación se denomina como la cota inferior de la evidencia (`ELBO`) del inglés **evidence lower bound**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Optimización</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación clave de la inferencia variacional es dada por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{ELBO}  & = \n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] \\\\\n",
    "&= \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "El proceso de optimización se basa en la segunda parte ecuación, por lo que usaremos al definición\n",
    "\n",
    "$$\n",
    "\\text{ELBO}  = \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "$$\n",
    "\n",
    "\n",
    "El término $\\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$ corresponde al modelo generativo en el problema. La interpretación estadística de este término es que el modelo generador toma muestras obtenidas de la salida del modelo latente $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$, el cual estamos aproximando con $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$. Es decir, se genera una muestra $\\mathbf{z} \\sim Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ y partir de esta se trata de reconstruir la entrada $\\mathbf{x}$.\n",
    "\n",
    "En el ejemplo propuesto, si se considera que se tiene vectores dicotómicos, se asume una distribución de Bernoulli para cada componente. Si las respuestas son condicionalmente independientes dado el vector $\\mathbf{z}$ entonces la función de pérdida es la entropía cruzada binaria $\\mathcal{L}_R$ dada por\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\mathcal{L}_R = - \\frac{1}{d}\\sum_{j=1}^d x_j \\log p(\\mathbf{w}_j'\\mathbf{z} + b_j) + (1-x_j)\\log(1-p(\\mathbf{w}_j'\\mathbf{z} + b_j))}\n",
    "$$\n",
    "\n",
    "\n",
    "El segundo término $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]$ puede ser evaluado directamente. Como asumimos que $Q_{\\phi}$ es una distribución Gaussiana y si se tiene en cuenta que típicamente $P_{\\theta}(\\mathbf{z})= P(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, se obtiene que \n",
    "\n",
    "$$\n",
    "\\large{\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]= \\frac{1}{2} \\sum_{j=1}^{d} (1+\\log(\\sigma_j)^2 - (\\mu_j)^2-(\\sigma_j)^2)}\n",
    "$$\n",
    "\n",
    "Tanto $\\mu_j$ como $\\sigma_j$ son funciones de la entrada $\\mathbf{x}$ ue se estiman en el modelo de inferencia.\n",
    "\n",
    "Para minimizar $\\mathcal{L}_{KL} =D_{KL}$, se requiere que $\\mu_j\\to 0$ y $\\sigma_j\\to 1$.\n",
    "\n",
    "En resumen para el problema de inferencia variacional la función de pérdida es dada por\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\mathcal{L}_{VAE} = \\mathcal{L}_{R} + \\mathcal{L}_{KL}}\n",
    "$$\n",
    "\n",
    "[Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
